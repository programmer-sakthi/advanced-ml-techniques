### **Day 1 – Supervised Learning: Naive Bayes**

**Session 1:**
Naive Bayes Overview – Probabilistic classifier based on Bayes’ Theorem, independence assumption, applications in text classification, spam detection, sentiment analysis

**Session 2:**
Gaussian Naive Bayes – Gaussian distribution for continuous data, assumptions, hyperparameter tuning, Python implementation

**Session 3:**
Multinomial Naive Bayes – Handling discrete/text data (Bag of Words, TF-IDF), model training and evaluation

**Session 4:**
Real-World Case Study – Problem statement, preprocessing, model implementation, tuning, evaluation, key lessons

---

### **Day 2 – Supervised Learning: Support Vector Machines (SVM)**

**Session 1:**
SVM Overview – Separating hyperplane, linear vs non-linear SVM

**Session 2:**
Kernel Tricks – Linear, Polynomial, RBF kernels, margins, support vectors

**Session 3:**
SVM Tuning & Evaluation – Hyperparameters (C, gamma, kernel), grid search, cross-validation, performance metrics

**Session 4:**
Text Classification using SVM – TF-IDF, Bag of Words, spam detection use case

---

### **Day 3 – Supervised Learning: Decision Trees**

**Session 1:**
Decision Tree Overview – Classification vs regression trees, feature-based splitting

**Session 2:**
Entropy & Gini Index – ID3 vs CART, impurity measures comparison

**Session 3:**
Tree Construction & Visualization – Recursive splitting, feature importance, Graphviz/Matplotlib

**Session 4:**
Overfitting & Pruning – Pre-pruning, post-pruning, controlling tree depth

---

### **Day 4 – Model Evaluation: Cross-Validation**

**Session 1:**
Introduction to Cross-Validation – Purpose, types (K-Fold, Hold-out, Stratified)

**Session 2:**
K-Fold Cross-Validation – Working principle and Scikit-learn implementation

**Session 3:**
Hold-out & Stratified K-Fold – Maintaining class distribution

**Session 4:**
Model Comparison – Accuracy, Precision, Recall, F1-Score using cross-validation

---

### **Day 5 – Ensemble Learning: Random Forest**

**Session 1:**
Random Forest Overview – Ensemble of decision trees, advantages

**Session 2:**
OOB Error & Feature Importance – Understanding unused samples and feature contribution

**Session 3:**
Training Random Forest – Number of trees, max depth, aggregation

**Session 4:**
Model Evaluation & Tuning – Performance metrics and hyperparameter tuning

---

### **Day 6 – Ensemble Learning: Boosting Techniques**

**Session 1:**
Boosting Concept – AdaBoost and weak learners

**Session 2:**
XGBoost Implementation – Hyperparameters (learning rate, depth, estimators)

**Session 3:**
Boosting for Classification – Handling imbalanced data

**Session 4:**
Performance Comparison – Boosting vs SVM & Random Forest

---

### **Day 7 – Unsupervised Learning: Hierarchical Clustering**

**Session 1:**
Hierarchical Clustering Overview – Agglomerative vs divisive

**Session 2:**
Dendrograms – Visualization and interpretation

**Session 3:**
Linkage Criteria – Single, complete, average, Ward’s method

**Session 4:**
Cluster Interpretation – Choosing optimal clusters

---

### **Day 8 – Unsupervised Learning: DBSCAN**

**Session 1:**
DBSCAN Overview – Density-based clustering, core, border, noise points

**Session 2:**
Epsilon-Neighborhood – Density-based grouping

**Session 3:**
Noise & Outliers – Handling irregular data

**Session 4:**
Parameter Tuning & Visualization – Epsilon (ε), MinPts

---

### **Day 9 – Dimensionality Reduction: PCA & LDA**

**Session 1:**
PCA Overview – Dimensionality reduction basics

**Session 2:**
PCA for Variance Preservation – Explained variance and component selection

**Session 3:**
LDA for Class Separation – Maximizing between-class variance

**Session 4:**
Visualization – Plotting reduced dimensions

---

### **Day 10 – GenAI Tools: ML Workflow Automation**

**Session 1:**
Introduction to GenAI Tools – Use cases and overview

**Session 2:**
AutoML Pipelines – Automated model selection and evaluation

**Session 3:**
Feature Engineering Automation – End-to-end ML workflow

**Session 4:**
Final Project – ML workflow automation using GenAI tools
