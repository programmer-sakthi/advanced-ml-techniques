Problem Statement 



A leading company is experiencing high employee attrition and wants to identify key factors influencing employee retention. The HR analytics team needs to explore and analyze the employee dataset to understand patterns related to satisfaction levels, performance evaluations, workload, compensation, and other factors that may impact whether employees leave the organization. 



As the first step in this analytical process, the team needs to: 



Load and inspect the employee dataset – Import the CSV file containing employee records and display the first 5 rows to understand the data structure. 
Determine dataset size – Identify the total number of employee records (samples) in the dataset to understand the scope of analysis. 
Examine data types – Display the data type of each column to ensure proper handling of numeric and categorical variables in subsequent analysis. 
Identify feature columns – List the specific feature columns that will be used for predictive modeling and classification tasks. These features include satisfaction_level, last_evaluation, number_project, average_montly_hours, time_spend_company, Work_accident, and promotion_last_5years. 
Generate statistical summary – Compute and display descriptive statistics (count, mean, standard deviation, min, max, quartiles) for all numeric columns to understand data distribution and identify potential outliers. 
Build and evaluate a Decision Tree model – Train a Decision Tree Classifier to predict employee attrition (whether an employee will leave), scale the features using StandardScaler, split data into training (80%) and testing (20%) sets, and evaluate model performance using accuracy score and classification report. 


This exploratory data analysis and predictive modeling forms the foundation for deeper insights into employee retention patterns and will guide HR strategies. 



 CSV File Structure :

﻿



Sample data :



satisfaction_level,last_evaluation,number_project,average_montly_hours,time_spend_company,Work_accident,left,promotion_last_5years,Department,salary

0.4,0.65,2,296,5,0,1,0,sales,medium

0.45,0.51,2,155,3,1,1,0,sales,medium

0.39,0.56,2,130,3,0,1,0,sales,medium

0.43,0.48,2,157,3,0,1,0,sales,medium

0.09,0.96,6,245,4,0,1,0,sales,medium

0.79,0.86,5,226,5,0,1,0,sales,medium

Input format :
CSV File Input:

The program prompts the user to enter the name of the CSV file containing employee data.
Input must include the file extension .csv.


Output format :
Program Output Sequence 



The program generates the following outputs in sequence: 



1. First 5 rows of the dataset: 



Displays a DataFrame showing the first 5 employee records 
Output includes all columns with proper formatting 
Ends with [5 rows x 10 columns] indicating dimensions 
2. Number of samples in the data: 



Prints the total count of employee records (rows) in the dataset 
Single integer value on its own line 
3. Data types of each column: 



Displays each column name followed by its data type 
Format: column_name dtype 
Ends with dtype: object on its own line 
4. Feature columns used for classification: 



Prints a Python list containing the names of feature columns 
List format: ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'Work_accident', 'promotion_last_5years'] 
5. Statistical summary of numeric columns: 



Displays descriptive statistics using pandas .describe() output format 
Includes: count, mean, std, min, 25%, 50%, 75%, max for each numeric column 
Shows [8 rows x 10 columns] indicating the summary dimensions 
6. Model Performance Metrics: 



Model Accuracy: Displays the accuracy score of the Decision Tree Classifier on test data (float value between 0 and 1) 
Classification Report: Displays precision, recall, f1-score, and support for each class (0 = Stayed, 1 = Left)  
Includes metrics for both classes 
Includes accuracy, macro avg, and weighted avg 
Formatted as a table with aligned columns 
 

Code constraints :
CSV File Constraints 



File must be a valid comma-separated CSV file with .csv extension 
File must exist in the same directory as the Python script 
File must contain a header row with exact column names (case-sensitive) 
All 10 columns must be present with exact names: satisfaction_level, last_evaluation, number_project, average_montly_hours, time_spend_company, Work_accident, left, promotion_last_5years, Department, salary 


Column Data Constraints 



satisfaction_level: Float values between 0.0 and 1.0 (inclusive) 
last_evaluation: Float values between 0.0 and 1.0 (inclusive) 
number_project: Non-negative integers representing project count 
average_montly_hours: Positive integers representing monthly work hours 
time_spend_company: Non-negative integers representing years at company 
Work_accident: Binary values (0 = No accident, 1 = Had accident) 
left: Binary values (0 = Stayed, 1 = Left the company) – Target Variable 
promotion_last_5years: Binary values (0 = Not promoted, 1 = Promoted) 
Department: Non-empty string values (e.g., sales, technical, support, IT, admin, management, marketing, accounting, hr, RandD, product_mng) 
salary: Must be one of three categorical values: low, medium, or high 


Program Constraints 



The program uses pandas, numpy, and scikit-learn libraries for data manipulation and machine learning 
If the CSV file is not found, the program prints an error message: "Error: File '{filename}' not found." and exits 
The .head() method displays exactly the first 5 rows of the dataset 
The shape attribute .shape[0] retrieves the total number of rows (samples) 
The .dtypes attribute displays data types for all columns in the format: column name followed by dtype 
The feature columns list is hardcoded and must match exactly: ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'Work_accident', 'promotion_last_5years'] 
The .describe() method generates statistical summaries for all numeric columns 
Feature Scaling: StandardScaler is applied to normalize feature values before model training 
Train-Test Split: Data is split with test_size=0.2 and random_state=42 
Model Configuration: DecisionTreeClassifier with random_state=42 and max_depth=4 
Evaluation Metrics: Uses accuracy_score and classification_report from sklearn.metrics 
All print statements must match the exact format shown in the sample output, including spacing and line breaks 
Warnings are suppressed using warnings.simplefilter(action='ignore') 


Input/Output Specifications 



Input: The program accepts one line of input containing the CSV filename 
Output: Sequential display of dataset information, statistics, and model performance metrics as described above 
 

Sample test cases :
Input 1 :
Sample.csv
Output 1 :
First 5 rows of the dataset:
   satisfaction_level  last_evaluation  ...  Department  salary
0                0.38             0.53  ...       sales     low
1                0.80             0.86  ...       sales  medium
2                0.11             0.88  ...       sales  medium
3                0.72             0.87  ...       sales     low
4                0.37             0.52  ...       sales     low

[5 rows x 10 columns]

Number of samples in the data:
49

Data types of each column:
satisfaction_level       float64
last_evaluation          float64
number_project             int64
average_montly_hours       int64
time_spend_company         int64
Work_accident              int64
left                       int64
promotion_last_5years      int64
Department                object
salary                    object
dtype: object

Feature columns:
['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'Work_accident', 'promotion_last_5years']

Statistical summary of numeric columns:
       satisfaction_level  last_evaluation  ...  left  promotion_last_5years
count           49.000000        49.000000  ...  49.0              49.000000
mean             0.454082         0.678571  ...   1.0               0.020408
std              0.252973         0.182779  ...   0.0               0.142857
min              0.090000         0.460000  ...   1.0               0.000000
25%              0.380000         0.530000  ...   1.0               0.000000
50%              0.410000         0.570000  ...   1.0               0.000000
75%              0.570000         0.870000  ...   1.0               0.000000
max              0.920000         1.000000  ...   1.0               1.000000

[8 rows x 8 columns]

Model Accuracy: 1.0

Classification Report:
              precision    recall  f1-score   support

           1       1.00      1.00      1.00        10

    accuracy                           1.00        10
   macro avg       1.00      1.00      1.00        10
weighted avg       1.00      1.00      1.00        10s