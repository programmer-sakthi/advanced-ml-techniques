Problem Statement



After building and training a machine learning model, it is critical to evaluate its performance on unseen test data to determine how well it generalizes. Model evaluation provides quantitative metrics that help assess the classifier's effectiveness in predicting employee attrition. Without proper evaluation, the team cannot confidently deploy the model or make data-driven decisions.



The HR analytics team needs to systematically evaluate the Naïve Bayes classifier's performance using multiple metrics and ensure that reusable evaluation functions are available for future modeling tasks.



The team needs to accomplish two key evaluation tasks:



1. Evaluate classifier performance using a pre-built module – Import and utilize the ML_Modules module that contains a reusable evaluate_classifier function:



The function takes actual test labels (y_test) and predicted labels (y_pred) as inputs.
It computes and displays multiple evaluation metrics:
Confusion Matrix: Shows the distribution of true positives, true negatives, false positives, and false negatives.
Classification Report: Provides precision, recall, and f1-score for each class, along with macro and weighted averages.
Individual Metrics: Displays accuracy, recall, f1-score, and precision with 3 decimal precision.
These metrics collectively provide a comprehensive view of model performance across different dimensions.


2. Verify availability of AUC-ROC evaluation function – Confirm that the ML_Modules.py file contains a reusable auc_roc function:



This function is designed to calculate the ROC AUC (Receiver Operating Characteristic - Area Under Curve) score, which measures the classifier's ability to distinguish between classes across different threshold values.
The function accepts three parameters: the trained classifier object, test features (X_test), and actual test labels (y_test).
The function computes predicted probabilities and calculates the ROC AUC score.
A confirmation message is displayed indicating that this function is available in the module for future use.


This structured evaluation approach ensures consistent performance measurement across different models and enables comparison of various classification algorithms.



CSV File Structure



﻿



Sample Data



satisfaction_level,last_evaluation,number_project,average_montly_hours,time_spend_company,Work_accident,left,promotion_last_5years,Department,salary

0.38,0.53,2,157,3,0,1,0,sales,low

0.8,0.86,5,262,6,0,1,0,sales,medium

0.11,0.88,7,272,4,0,1,0,sales,medium

0.72,0.87,5,223,5,0,1,0,sales,low

0.37,0.52,2,159,3,0,1,0,sales,low

Input format :
CSV File Input:

The program prompts the user to enter the name of the CSV file containing employee data.
Input must include the file extension .csv.
Output format :
The program generates evaluation metrics in the following sequence:



1. Confusion Matrix:



Header line: "Confusion Matrix"
Displays a 2D array showing the confusion matrix.
For binary classification:
If only one class appears in predictions, shows a single value (e.g., [[16]]).
If both classes appear, shows a 2x2 matrix with true negatives, false positives, false negatives, and true positives.
Followed by a separator line: "==================="


2. Classification Report:



Header line: "Classification Report:"
Displays a detailed table with the following columns: precision, recall, f1-score, support.
Rows include:
Per-class metrics (0 and/or 1).
accuracy: Overall accuracy across all classes.
macro avg: Unweighted average of per-class metrics.
weighted avg: Weighted average of per-class metrics (weighted by support).
All metric values are displayed with 2 decimal precision in the table.
Followed by a separator line: "==================="


3. Individual Performance Metrics:



Four lines displaying key metrics with exactly 3 decimal precision:
accuracy: X.XXX - Proportion of correct predictions.
recall: X.XXX - Weighted average recall (sensitivity).
f1-score: X.XXX - Weighted average F1 score (harmonic mean of precision and recall).
precision: X.XXX - Weighted average precision (positive predictive value).


4. AUC-ROC Function Availability Message:



Single line: "code is available inside the 'ML_Modules.py' file"
Confirms that the auc_roc function is defined and ready for use in the ML_Modules module.


Refer to the sample output for exact formatting specifications.

Code constraints :
CSV File Constraints



File must be a valid comma-separated CSV file with .csv extension.
File must exist in the same directory as the Python script.
File must contain a header row with exact column names (case-sensitive).
All expected columns must be present: satisfaction_level, last_evaluation, number_project, average_montly_hours, time_spend_company, Work_accident, left, promotion_last_5years, Department, salary
The left column must contain binary values (0 or 1) as the target variable.


Module Requirements



ML_Modules.py file must exist in the same directory as the main script.
Required imports in ML_Modules.py:
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score
Required function: evaluate_classifier(y_test, y_pred)
Parameters: actual test labels (y_test), predicted labels (y_pred).
Must compute and display: confusion matrix, classification report, accuracy, recall, f1-score, precision.
Metrics must be formatted with 3 decimal precision.
Required function: auc_roc(classifier, X_test, y_test)
Parameters: trained classifier object, test features (X_test), actual test labels (y_test).
Must calculate ROC AUC score using predicted probabilities.
The classifier must support the predict_proba() method.


Column Data Constraints



satisfaction_level, last_evaluation: Float values (typically 0.0 to 1.0).
number_project, average_montly_hours, time_spend_company: Integer values.
Work_accident, left, promotion_last_5years: Binary integer values (0 or 1).
Department: Non-empty string values representing department names.
salary: Non-empty string values (low, medium, high).
No missing values should be present for proper model training and evaluation.


Program Constraints



The program must import the ML_Modules module.
If the CSV file is not found, the program prints: "Error: File '{filename}' not found." and exits.
Data preprocessing pipeline:
Categorical variables (Department, salary) must be converted to dummy variables using one-hot encoding.
Original categorical columns must be dropped after dummy creation.
All features must be numeric for Naïve Bayes classifier compatibility.


Train-test split:



Split ratio: 70% training, 30% testing.
Features (X) are all columns except left.
Target (y) is the left column.


Model training:



Uses Gaussian Naïve Bayes classifier.
Must be trained on X_train and y_train before evaluation.


Evaluation requirements:



Predictions (y_pred) must be generated using the trained model on X_test.
The evaluate_classifier function must be called with y_test and y_pred as arguments.
Evaluation metrics are computed by comparing predicted labels against actual labels.


Evaluation Metrics Explanation



Confusion Matrix:

For binary classification with classes 0 and 1:
True Negatives (TN): Correctly predicted class 0.
False Positives (FP): Incorrectly predicted class 1 when actual is 0.
False Negatives (FN): Incorrectly predicted class 0 when actual is 1.
True Positives (TP): Correctly predicted class 1.
Matrix structure: [[TN, FP], [FN, TP]]
If test set contains only one class, matrix may be 1x1.


Classification Report Metrics:



Precision: TP / (TP + FP) - Of all positive predictions, how many were correct?
Recall (Sensitivity): TP / (TP + FN) - Of all actual positives, how many were identified?
F1-Score: 2 × (Precision × Recall) / (Precision + Recall) - Harmonic mean of precision and recall.
Support: Number of actual occurrences of each class in the test set.
Macro avg: Simple average of per-class metrics (treats all classes equally).
Weighted avg: Average of per-class metrics weighted by support (accounts for class imbalance).


Individual Metrics Displayed:



Accuracy: (TP + TN) / Total - Overall proportion of correct predictions.
Recall, F1-score, Precision: Weighted averages from the classification report.
All displayed with exactly 3 decimal places (e.g., 0.875 displayed as "0.875", 1.0 displayed as "1.000").
Sample test cases :
Input 1 :
Sample.csv
Output 1 :
Confusion Matrix
[[16]]
===================
Classification Report:
              precision    recall  f1-score   support

           1       1.00      1.00      1.00        16

    accuracy                           1.00        16
   macro avg       1.00      1.00      1.00        16
weighted avg       1.00      1.00      1.00        16

===================
accuracy: 1.000
recall: 1.000
f1-score: 1.000
precision: 1.000
code is available inside the 'ML_Modules.py' file
